{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 - Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Lab 2: Ridge Regression](#6.6.1-Ridge-Regression)\n",
    "- [Lab 2: The Lasso](#6.6.2-The-Lasso)\n",
    "- [Lab 3: Principal Components Regression](#6.7.1-Principal-Components-Regression)\n",
    "- [Lab 3: Partial Least Squares](#6.7.2-Partial-Least-Squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In R, I exported the dataset from package 'ISLR' to a csv file.\n",
    "df = pd.read_csv('Data/Hitters.csv', index_col=0).dropna()\n",
    "df.index.name = 'Player'\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\n",
    "dummies.info()\n",
    "print(dummies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I executed the R code and downloaded the exact same training/test sets used in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Data/Hitters_X_train.csv', index_col=0)\n",
    "y_train = pd.read_csv('Data/Hitters_y_train.csv', index_col=0)\n",
    "X_test = pd.read_csv('Data/Hitters_X_test.csv', index_col=0)\n",
    "y_test = pd.read_csv('Data/Hitters_y_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.1 Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __glmnet__ algorithms in R optimize the objective function using cyclical coordinate descent, while scikit-learn Ridge regression uses linear least squares with L2 regularization. They are rather different implementations, but the general principles are the same.\n",
    "\n",
    "The __glmnet() function in R__ optimizes:\n",
    "### $$ \\frac{1}{N}|| X\\beta-y||^2_2+\\lambda\\bigg(\\frac{1}{2}(1âˆ’\\alpha)||\\beta||^2_2 \\ +\\ \\alpha||\\beta||_1\\bigg) $$\n",
    "(See R documentation and https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.pdf)<BR>\n",
    "The function supports L1 and L2 regularization. For just Ridge regression we need to use $\\alpha = 0 $. This reduces the above cost function to\n",
    "### $$ \\frac{1}{N}|| X\\beta-y||^2_2+\\frac{1}{2}\\lambda ||\\beta||^2_2 $$\n",
    "The __sklearn Ridge()__ function optimizes:\n",
    "### $$ ||X\\beta - y||^2_2 + \\alpha ||\\beta||^2_2 $$\n",
    "which is equivalent to optimizing\n",
    "### $$ \\frac{1}{N}||X\\beta - y||^2_2 + \\frac{\\alpha}{N} ||\\beta||^2_2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "ridge = Ridge()\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha=a)\n",
    "    ridge.fit(scale(X), y)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge coefficients as a function of the regularization');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows that the Ridge coefficients get larger when we decrease alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpha = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge2 = Ridge(alpha=len(X_)*11498/2)\n",
    "ridge2.fit(scaler.transform(X_train), y_train)\n",
    "pred = ridge2.predict(scaler.transform(X_test))\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ridge2.coef_.flatten(), index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpha = $10^{10}$ \n",
    "This big penalty shrinks the coefficients to a very large degree and makes the model more biased, resulting in a higher MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge2.set_params(alpha=10**10)\n",
    "ridge2.fit(scale(X_train), y_train)\n",
    "pred = ridge2.predict(scale(X_test))\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the regularization path using RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error')\n",
    "ridgecv.fit(scale(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge2.set_params(alpha=ridgecv.alpha_)\n",
    "ridge2.fit(scale(X_train), y_train)\n",
    "mean_squared_error(y_test, ridge2.predict(scale(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ridge2.coef_.flatten(), index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.2 The Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For both __glmnet__ in R and sklearn __Lasso()__ function the standard L1 penalty is:\n",
    "### $$ \\lambda |\\beta|_1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter=10000)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas*2:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(scale(X_train), y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Lasso coefficients as a function of the regularization');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassocv = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "lassocv.fit(scale(X_train), y_train.as_matrix().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassocv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(scale(X_train), y_train)\n",
    "mean_squared_error(y_test, lasso.predict(scale(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the coefficients are now reduced to exactly zero.\n",
    "pd.Series(lasso.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.1 Principal Components Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-klearn does not have an implementation of PCA and regression combined like the 'pls' package in R.\n",
    "https://cran.r-project.org/web/packages/pls/vignettes/pls-manual.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_reduced = pca.fit_transform(scale(X))\n",
    "\n",
    "print(pca.components_.shape)\n",
    "pd.DataFrame(pca.components_.T).loc[:4,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above loadings are the same as in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_reduced.shape)\n",
    "pd.DataFrame(X_reduced).loc[:4,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above principal components are the same as in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance explained by the principal components\n",
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold CV, with shuffle\n",
    "n = len(X_reduced)\n",
    "kf_10 = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "regr = LinearRegression()\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "score = -1*cross_val_score(regr, np.ones((n,1)), y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "\n",
    "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
    "for i in np.arange(1, 20):\n",
    "    score = -1*cross_val_score(regr, X_reduced[:,:i], y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "    \n",
    "plt.plot(mse, '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot indicates that the lowest training MSE is reached when doing regression on 18 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_test = LinearRegression()\n",
    "regr_test.fit(X_reduced, y)\n",
    "regr_test.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting PCA with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA()\n",
    "X_reduced_train = pca2.fit_transform(scale(X_train))\n",
    "n = len(X_reduced_train)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = KFold(n_splits=10, shuffle=False, random_state=1)\n",
    "\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "score = -1*cross_val_score(regr, np.ones((n,1)), y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "\n",
    "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
    "for i in np.arange(1, 20):\n",
    "    score = -1*cross_val_score(regr, X_reduced_train[:,:i], y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "\n",
    "plt.plot(np.array(mse), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot indicates that the lowest training MSE is reached when doing regression on 6 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform test data with PCA loadings and fit regression on 6 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_test = pca2.transform(scale(X_test))[:,:7]\n",
    "\n",
    "# Train regression model on training data \n",
    "regr = LinearRegression()\n",
    "regr.fit(X_reduced_train[:,:7], y_train)\n",
    "\n",
    "# Prediction with test data\n",
    "pred = regr.predict(X_reduced_test)\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.2 Partial Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn PLSRegression gives same results as the pls package in R when using 'method='oscorespls'. In the LAB excercise, the standard method is used which is 'kernelpls'. \n",
    "\n",
    "When doing a slightly different fitting in R, the result is close to the one obtained using scikit-learn.\n",
    "\n",
    "    pls.fit=plsr(Salary~., data=Hitters, subset=train, scale=TRUE, validation=\"CV\", method='oscorespls')\n",
    "    validationplot(pls.fit,val.type=\"MSEP\", intercept = FALSE)\n",
    "   \n",
    "See documentation:\n",
    "http://scikit-learn.org/dev/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X_train)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = KFold(n_splits=10, shuffle=False, random_state=1)\n",
    "\n",
    "mse = []\n",
    "\n",
    "for i in np.arange(1, 20):\n",
    "    pls = PLSRegression(n_components=i)\n",
    "    score = cross_val_score(pls, scale(X_train), y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(-score)\n",
    "\n",
    "plt.plot(np.arange(1, 20), np.array(mse), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls = PLSRegression(n_components=2)\n",
    "pls.fit(scale(X_train), y_train)\n",
    "\n",
    "mean_squared_error(y_test, pls.predict(scale(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
